import os
import datetime
import feedparser
import requests
import json
import time

# --- è¨­å®š ---
API_KEY = os.environ.get("GEMINI_API_KEY", "").strip()
MODEL_NAME = "gemini-2.0-flash-lite"
API_URL = f"https://generativelanguage.googleapis.com/v1/models/{MODEL_NAME}:generateContent?key={API_KEY}"

POSTS_DIR = "src/pages/posts"
os.makedirs(POSTS_DIR, exist_ok=True)

# Googleãƒ‹ãƒ¥ãƒ¼ã‚¹çµŒç”±ã§ã€ŒãƒšãƒŠãƒ³ã€ã¨ã€Œãƒãƒ¬ãƒ¼ã‚·ã‚¢ã€ã®æœ€æ–°æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã«ãã„ï¼‰
RSS_URLS = [
    "https://news.google.com/rss/search?q=Penang+when:24h&hl=en-MY&gl=MY&ceid=MY:en",
    "https://news.google.com/rss/search?q=Malaysia+Education+when:24h&hl=en-MY&gl=MY&ceid=MY:en"
]

def ask_ai(title, summary, link):
    print(f"AIç¿»è¨³ä¾é ¼ä¸­: {title[:30]}...")
    prompt = f"ä»¥ä¸‹ã®è‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒšãƒŠãƒ³åœ¨ä½æ—¥æœ¬äººå‘ã‘ã«ç¿»è¨³ãƒ»æ•´å½¢ã—ã¦ã€‚1è¡Œç›®ã¯ã€Œã‚¸ãƒ£ãƒ³ãƒ«ï¼šã€‡ã€‡ã€ã¨ã—ã¦ã€‚ã‚¿ã‚¤ãƒˆãƒ«: {title}, å†…å®¹: {summary}"
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    try:
        response = requests.post(API_URL, headers={'Content-Type': 'application/json'}, data=json.dumps(payload), timeout=30)
        if response.status_code == 200:
            data = response.json()
            content = data["candidates"][0]["content"]["parts"][0]["text"]
            lines = content.strip().split('\n')
            genre = "æš®ã‚‰ã—"
            if "ã‚¸ãƒ£ãƒ³ãƒ«ï¼š" in lines[0]:
                genre = lines[0].replace("ã‚¸ãƒ£ãƒ³ãƒ«ï¼š", "").strip()
                body = "\n".join(lines[1:])
            else:
                body = content

            return f"""---
title: "{title}"
date: "{datetime.date.today()}"
category: "{genre}"
---
<div class="genre-label">ã‚¸ãƒ£ãƒ³ãƒ«ï¼š{genre}</div>
<h3>ã€å†…å®¹ã€‘</h3>

{body}

<a href="{link}" target="_blank" rel="noopener noreferrer" class="source-link">ğŸ”— å‚ç…§å…ƒè¨˜äº‹ã‚’ç¢ºèªã™ã‚‹</a>
"""
    except:
        return None

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
print(f"PJN Bot èµ·å‹• (Google News RSSä½¿ç”¨)")
count = 0

for url in RSS_URLS:
    if count >= 3: break
    print(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹: {url}")
    feed = feedparser.parse(url)
    print(f"å–å¾—çµæœ: {len(feed.entries)}ä»¶ç™ºè¦‹")

    for entry in feed.entries:
        if count >= 3: break
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åä½œæˆ
        safe_title = "".join([c for c in entry.title if c.isalnum() or c==' '])[:30].strip().replace(" ", "_")
        filename = os.path.join(POSTS_DIR, f"{datetime.date.today()}-{safe_title}.md")
        
        if os.path.exists(filename): continue

        result = ask_ai(entry.title, entry.summary, entry.link)
        if result:
            with open(filename, "w", encoding="utf-8") as f:
                f.write(result)
            print(f"âœ… ä¿å­˜å®Œäº†: {filename}")
            count += 1
            time.sleep(60)

print(f"å®Œäº†ã€‚ä½œæˆè¨˜äº‹æ•°: {count}")

import os
import datetime
import feedparser
import google.generativeai as genai
import time

# --- AIè¨­å®š ---
genai.configure(api_key=os.environ["GEMINI_API_KEY"])

# è©¦è¡Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆå‹•ãã‚‚ã®ã‚’è‡ªå‹•ã§æ¢ã—ã¾ã™ï¼‰
MODELS_TO_TRY = ["gemini-1.5-flash", "gemini-1.5-flash-latest", "gemini-pro"]

POSTS_DIR = "src/pages/posts"
os.makedirs(POSTS_DIR, exist_ok=True)

RSS_URLS = [
    "https://www.thestar.com.my/rss/news/nation",
    "https://www.thestar.com.my/rss/metro/community"
]

def ask_ai(title, summary, link):
    prompt = f"""
    ã‚ãªãŸã¯ãƒšãƒŠãƒ³åœ¨ä½æ—¥æœ¬äººå‘ã‘ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ç·¨é›†é•·ã§ã™ã€‚
    ä»¥ä¸‹ã®è‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã€å­è‚²ã¦ä¸–å¸¯ã‚„æ¯å­ç•™å­¦ç”ŸãŒèª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã«å…¨æ–‡ç¿»è¨³ãƒ»æ•´å½¢ã—ã¦ãã ã•ã„ã€‚

    ã‚¿ã‚¤ãƒˆãƒ«: {title}
    å†…å®¹: {summary}

    ã€å‡ºåŠ›ãƒ«ãƒ¼ãƒ«ã€‘
    1. å†’é ­ã«ã€Œã‚¸ãƒ£ãƒ³ãƒ«ï¼šã€‡ã€‡ã€ã‚’æ˜è¨˜
    2. ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œã€ã‚¸ãƒ£ãƒ³ãƒ«ã€‘ã‚¿ã‚¤ãƒˆãƒ«ã€ã®å½¢å¼ã«ã€‚
    3. æœ¬æ–‡ã¯3-4è¡Œã”ã¨ã«æ”¹è¡Œã‚’å…¥ã‚Œã€èª­ã¿ã‚„ã™ãã€‚
    4. æœ€å¾Œã«ã€ŒğŸ”— å‚ç…§å…ƒè¨˜äº‹ã‚’ç¢ºèªã™ã‚‹ã€ã¨ã„ã†ãƒªãƒ³ã‚¯ã‚’ã¤ã‘ã‚‹ã€‚
    5. å‡ºåŠ›ã¯ä»¥ä¸‹ã®Markdownå½¢å¼ã®ã€Œä¸­èº«ã€ã ã‘ã‚’å‡ºåŠ›ã€‚
    ---
    title: "{title}"
    date: "{datetime.date.today()}"
    category: "ãƒ‹ãƒ¥ãƒ¼ã‚¹"
    ---
    <div class="genre-label">ã‚¸ãƒ£ãƒ³ãƒ«ï¼šãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
    <h3>ã€å†…å®¹ï¼ˆå…¨æ–‡ç¿»è¨³ï¼‰ã€‘</h3>
    """

    for model_name in MODELS_TO_TRY:
        try:
            print(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã§è©¦è¡Œä¸­...")
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt + "\n\n(ç¿»è¨³ã•ã‚ŒãŸæœ¬æ–‡ã‚’ã“ã“ã«)\n\n<a href='" + link + "' class='source-link'>ğŸ”— å‚ç…§å…ƒè¨˜äº‹ã‚’ç¢ºèªã™ã‚‹</a>")
            
            if response.text:
                return response.text
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue # æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
            
    return None

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
print("ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹...")
articles_count = 0

for url in RSS_URLS:
    feed = feedparser.parse(url)
    print(f"ã‚½ãƒ¼ã‚¹å–å¾—: {url} (è¨˜äº‹æ•°: {len(feed.entries)})")
    
    for entry in feed.entries[:5]: 
        if articles_count >= 10: break
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åä½œæˆ
        clean_title = "".join([c for c in entry.title if c.isalnum() or c==' '])[:30].strip().replace(" ", "_")
        filename = os.path.join(POSTS_DIR, f"{datetime.date.today()}-{clean_title}.md")
        
        if os.path.exists(filename): continue

        article_md = ask_ai(entry.title, entry.summary, entry.link)
        
        if article_md:
            with open(filename, "w", encoding="utf-8") as f:
                f.write(article_md)
            print(f"ä¿å­˜å®Œäº†: {filename}")
            articles_count += 1
        
        time.sleep(2) # APIåˆ¶é™å›é¿ã®ãŸã‚ã®å¾…æ©Ÿ

print(f"æœ¬æ—¥ã®æ¥­å‹™çµ‚äº†ã€‚ä½œæˆè¨˜äº‹æ•°: {articles_count}")
